diff --git a/c4rl.py b/c4rl.py
index d297f8d..63300f1 100644
--- a/c4rl.py
+++ b/c4rl.py
@@ -72,7 +72,7 @@ except OSError:
 # slows down training quite a lot. You can always safely abort the training prematurely using
 # Ctrl + C.
 #dqn.fit(env, nb_steps=5000000, visualize=False, verbose=2)
-sarsa.fit(env, nb_steps=50000, visualize=False, verbose=2, callbacks=[WandbCallback()])
+sarsa.fit(env, nb_steps=500, visualize=False, verbose=2, callbacks=[WandbCallback()])
 
 # After training is done, we save the final weights.
 #dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)
diff --git a/randomenv.py b/randomenv.py
index 8bfb672..9b17fab 100644
--- a/randomenv.py
+++ b/randomenv.py
@@ -20,6 +20,9 @@ class RandomEnv(C4Env):
             self.game.play(self.game.random_action(), self.game.current_player) 
             self.game.get_status()
         self.steps += 1
+
+        print(self.boardchecker())
+
         reward = float(-self.game.winner) / self.steps 
 
         done = self.game.get_status()
@@ -37,4 +40,7 @@ class RandomEnv(C4Env):
         self.steps = 0
         if (self.game.current_player==1): # so ist immer der RL Agent als erstes im step dran und kann keinen illegalen zug machen wenn er als action das selbe feld w√§hlt wie der Gegner
             self.game.play(self.game.random_action(), self.game.current_player) 
-        return np.array(self.game.board)
\ No newline at end of file
+        return np.array(self.game.board)
+
+    def boardchecker(self):
+        return self.game.check_for_streak(-1,2),self.game.check_for_streak(-1,3)
\ No newline at end of file
diff --git a/results/dqn_weights.h5f b/results/dqn_weights.h5f
index eb75ba0..3e8e27d 100644
Binary files a/results/dqn_weights.h5f and b/results/dqn_weights.h5f differ
