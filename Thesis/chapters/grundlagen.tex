\chapter{Grundlagen}%

\label{cha:Schluss}

In dieser Arbeit werden einige Technologien genutzt, um die Problemstellung zu lösen.
Die wichtigsten dieser Technologien werden hier erklärt, um das weitere Verständnis des Lesers zu gewährleisten. 


\section{Vier Gewinnt}
Vier Gewinnt ist ein Spiel für zwei Spieler, in dem abwechselnd Spielsteine in ein vertikales Spielfeld der Größe $7 X 6$ (42 Felder) eingeworfen werden. Jeder Spieler kann sich in seinem Zug zwischen einem von sieben Einwurflöchern entscheiden. Wird ein Spielstein eingeworfen, so fällt dieser auf die niedrigste der sechs Positionen, die noch nicht durch andere Spielsteine gefüllt ist. Hierdurch ergibt sich ein Spielfeld mit 42 Feldern.  Sind schon sechs Spielsteine in ein bestimmtes Einwurfloch gesteckt worden, so darf hier kein weiterer Spielstein eingeworfen werden. Ein Spieler gewinnt das Spiel, wenn er es geschafft  hat, dass sich vier seiner Spielsteine in einer Reihe befinden. Eine Reihe kann horizontal, vertikal oder diagonal gebildet werden. Werden alle 42 Positionen mit Spielsteinen befüllt, ohne eine Reihe von vier Steinen des selben Spielers zu bilden, geht die Partie unentschieden aus.
\begin{figure}[h!]
  \includegraphics[scale=0.1]{echtes_spiel.png}
  \centering
  \caption{Das Spiel Vier Gewinnt \cite{connect4}}
  \label{fig:echtes_spiel}
\end{figure}


\section{Minimax-Algorithmus}
Ein Minimax-Algorithmus beschreibt einen rekursiven Ansatz zum Finden einer optimalen Lösung für Probleme von zwei Parteien mit widersetzlichen Zielen. Dies sind in der Regel Nullsummenspiele mit perfekter Information, es gibt also für jeden Gewinn des einen Spielers genauso viel Verlust für den anderen und es gibt kein Spielelement, das nur für einen Spieler einsehbar ist. Der Minimax-Algorithmus berechnet sich hierfür den Suchbaum des Spiels, welcher alle möglichen Züge beider Spieler in nachvollziehbarer Reihenfolge enthält. Hierbei werden in jedem Rekursionsschritt alle möglichen Züge des derzeitigen Spielers betrachtet und bewertet. Wenn der derzeitige Zug nicht zum Gewinn führt, findet eine Bewertung durch das Betrachten des nächsten Rekursionsschrittes statt. 
Der Name Minimax ergibt sich durch das Betrachten der abwechselnden besten Züge der Spieler. Da dies aus der Perspektive von einem Spieler betrachtet wird, ergibt sich hieraus ein abwechselnd minimaler und maximaler Zug. Dieses Verfahren führt bei einfachen Spielen wie Tic-Tac-Toe zu einem perfekten Spieler\cite{borovska2007efficiency}, da hier eine überschaubare Menge an möglichen Zügen betrachtet wird (9 Felder, die von 2 Spielern gefüllt werden). Für komplexere Probleme gibt es die Möglichkeit eine Suchtiefe zu bestimmen, wodurch die Rekursion  nur bis zu dieser Tiefe durchgeführt wird. Dies ist sinnvoll da die Berechnung einzelner Züge sonst sehr lange dauern kann. Der Minimax-Algorithmus kann in bestimmten Spielen noch verbessert werden. Diese Verbesserungen sind der Negamax-Algorithmus und die Alpha-Beta-Suche, welche im Folgenden einmal beschrieben werden.  Der Einfachheit halber wird trotz dieser Erweiterungen in dieser Arbeit im Bezug auf diesen Algorithmus immer von Minimax geredet.\cite{borovska2007efficiency}

\subsection{Negamax-Algorithmus}
Der Negamax-Algorithmus basiert auf der Annahme, dass der maximal beste Zug für den einen Spieler der minimal beste Zug für den anderen bedeutet. Ist dies so in einem Spiel, in dem beide Spieler in jedem Zug die selben Dinge tun können, kann hierdurch die Rekursion vereinfacht werden, indem für beide Spieler die selbe Formel benutzt wird. Hierfür muss dann in jedem Rekursionsschritt der Input negiert werden. Wegen dieser Negierung heißt dieser Algorithmus auch Negamax.\cite{baier2006alpha}

\subsection{Alpha-Beta-Suche}
Die Alpha-Beta Suche ist eine Variante des Minimax-Algorithmus, die die Berechnung des Suchbaumes beschleunigt, indem sie bestimmte Teile der Suche nicht ausführt, wenn es schon einen deutlich besseren Pfad gibt.\cite{abdelbar2012alpha}
\newpage

\subsection{Wahrscheinlichkeitsverteilung}
Da der Minimax-Algorithmus im Normalfall immer deterministisch den besten Zug wählt, wurde er noch um eine Wahrscheinlichkeitsverteilung ergänzt. 
Die Wahrscheinlichkeitsverteilung basiert auf der Annahme, dass in jedem State (siehe \ref{state}) die Chancen der Menge aller möglichen Aktionen addiert 100 Prozent ergibt. Die Chancen der durch den Minimax-Algorithmus errechneten besseren Züge sind hierbei höher als die der schlechteren. Somit kann dann z.B. eine zufällige Aktion genommen werden, die aber sehr stark Züge mit besseren Chancen bevorzugt. Hierdurch wird der Minimax-Algorithmus probabilistisch und kann somit in seiner Stärke angepasst werden.\cite{diederich2003simple}

\section{Reinforcement-Learning}
Das Reinforcement-Learning (Bestärkendes Lernen) ist eine Machine-Learning Methode, bei der ein Agent mit Hilfe eines Environments, einer Policy, States, Actions und eines Rewards lernt, eine ihm gegebene Aufgabe zu lösen. Hierfür bieten sich Probleme an, die als Markow-Entscheidungsproblem formuliert werden können, da diese alles abdecken, was für ein Environment wichtig ist.\cite{Martijn2012} Es wird dabei grundsätzlich wie in Abbildung \ref{fig:rl_basic} dargestellt vorgegangen. Der Agent bekommt also einen State vom Environment, und durch diesen sucht  er sich mittels seiner Policy eine Action aus. Diese Action beeinflusst dann das Environment und dieses gibt dem Agenten einen Reward für seine Action. Durch den Reward kann der Agent dann die Gewichtungen für seine Policy anpassen. Dazu bekommt der Agent dann den neuen State und kann dann, mit der angepassten Policy, wieder eine neue Action auswählen. Diese Teile des Reinforcement-Learnings werden im Folgenden einmal genauer beschrieben.\cite{HandsOn2017, Sutton2018}\\

\begin{figure}[h!]
  \includegraphics[scale=0.5]{rl_basics.png}
  \centering
  \caption{grundsätzliches Konzept von reinforcement learning}
  \label{fig:rl_basic}
\end{figure}

\subsection{Environment}
Das Environment beschreibt das gesamte Problem, das der Agent versucht zu lösen. Es beinhaltet alles, was für das Spielen wichtig ist außer dem Spieler, welcher von dem Agenten behandelt wird.  Dies sind bei den meisten Spielen ein Spielfeld mit den Positionen aller Akteure, Spielsteine oder sonstigen Elemente, die zum Spielen genutzt werden. Dazu kümmert sich das Environment auch noch um die Spiellogik. Die Spiellogik beschreibt alle Abläufe, die durch die Regeln des Spiels definiert wurden. In dem Beispiel Vier Gewinnt wäre das unter anderem die Funktion, dass die Spielsteine immer auf das untereste freie Feld fallen oder die Überprüfung, ob ein Spieler gewonnen hat. Das letzte wichtige Element im Environment der meisten Spiele ist der Gegenspieler, welcher normalerweise die selben Möglichkeiten besitzt wie der Spieler. Für das Verhalten des Gegenspielers werden im Normalfall Algorithmen genutzt, die den bestmöglichen Zug errechnen, wie hier der Minimax-Algorithmus. Für sehr komplexe Spiele, bei denen sich kein optimaler Zug berechnen lässt, wird oft auf menschliche Spieler zurückgegriffen. Da ein Computer aber viel schneller im Spielen ist und Reinforcement-Learning viele Durchläufe absolvieren muss um Wirkung zu zeigen, wird versucht dies zu vermeiden oder bestehende Datensätze von Spielen mit Menschen zu nutzen.\\


\subsection{Agent}
Bei dem Agenten handelt es sich um den Akteur, der die ihm gegebene Aufgabe meistern soll. Er sieht das Environment und wählt mittels der Policy die ihm am besten erscheinende Action aus, um den Reward zu maximieren.\\

\subsection{Policy}
Mit der Policy wird das Verhalten beschrieben, nach dem der Agent entscheidet, welche der möglichen Aktionen die aktuell beste ist, um den Reward zu maximieren.\\

\subsection{State} \label{state}
Der State beschreibt den aktuellen Zustand des Environments, welcher in jedem Zug  an den Agenten weitergereicht wird, damit dieser seine Auswahl treffen kann. 

\subsection{Actions}
Actions sind die möglichen Aktionen, zwischen denen sich der Agent je nach State entscheiden muss. Diese sind alle möglichen Züge, die ein Spieler zu einem bestimmten Zeitpunkt ausführen kann.

\subsection{Reward}
Der Reward ist die Rückmeldung, die der Agent für seine Aktionen bekommt. Diese Rückmeldung kann positiv aber auch negativ sein und beschreibt, wie gut es ist eine bestimmte Aktion in einem bestimmten State auszuführen. Hieran passt der Agent dann während des Trainings seine Q-Values (siehe  \ref{qvalue}) entsprechend seiner Policy an. Der Reward wird also benutzt, um das Verhalten des Agenten zu beeinflussen. Möchte man ein bestimmtes Verhalten erzielen, wird hierfür ein positiver Reward vergeben. Möchte man ein Verhalten vermeiden, wird ein negativer Reward vergeben. Je besser die  Reward-Funktion gewählt wird, desto eher wird der Agent gewolltes Verhalten zeigen. Bei der Wahl einer Reward-Funktion wird zwischen diskreten und  kontinuierlichen Funktionen unterschieden. Es ist aber auch eine Mischung dieser beiden möglich.\\
Bei einer diskreten Reward-Funktion werden nur feste, im Vorhinein definierte Rewards gegeben, die zu bestimmten Events verteilt werden. Solch ein Event kann zum Beispiel das Gewinnen und Verlieren am Ende eines Spiels sein oder das Betreten des Agenten in einen gewollten oder ungewollten Bereich. \\
Bei einer kontinuierlichen Reward-Funktion wird der Reward zu jedem Schritt durch verschiedene Faktoren aus dem Environment berechnet. Solche Faktoren können zum Beispiel die verstrichene Zeit, die Anzahl der Züge oder die aktuelle Position des Agenten sein. Eine gute kontinuierliche Reward-Funktion hilft dabei, den Agenten schneller dem gewünschten Verhalten anzunähern, indem der Reward zu den gewünschten Zielstates hin immer weiter angehoben wird.\\ 
Da diese beiden Arten von Reward-Funktionen das Verhalten des Agenten an unterschiedlichen Stellen beeinflussen, ist es oft sinnvoll eine Mischung dieser zu nutzen.
Wird sich für eine Mischung entschieden ist es wichtig, dass die aus beiden Teilen kommenden Rewards in Relation zueinander stehen. Dies kann sonst dazu führen, dass die Auswirkung von dem Teil mit kleineren Rewards von dem Teil mit größeren Rewards überschattet wird.\\

\subsection{Q-Funktion}
Bei komplexeren Environments ist es schwer für jeden Zug vorauszusehen wie zielführend er ist. Um diesem Credit Assignment Problem(siehe \ref{cap}) entgegenzuwirken wird eine Q-Funktion benutzt, die anhand der gelernten Zustände versucht die ungenauen Q-Values an ihre potenziellen Werte anzunähern.\cite{Sutton2018}

\subsection{Q-Values} \label{qvalue}
Die Q-Values beschreiben für jedes Paar von State und Action einen erwarteten Reward. Leider sind diese Q-Values für die meisten Probleme nicht bekannt und müssen daher mit einer Q-Funktion antrainiert werden.\cite{Sutton2018}

\subsection{Deep Q-Network}
Da es bei einem Spiel mit einer Komplexität wie Vier Gewinnt nicht möglich ist für alle Paare von State und Action einen erwarteten Reward zu bestimmen, wird ein künstliches neuronales Netz genutzt, welches versucht eine Q-Funktion für diese erwarteten Rewards zu bestimmen. Dieses Verfahren nennt sich Deep Q-Learning. Hierbei wird ein Deep Q-Network erstellt, welches während des Trainings eine Policy erstellt. Hierfür werden alle gespielten Spiele abgespeichert. Von diesen Spielen werden dann zufällige Züge betrachtet und somit die aktuelle Policy durch die hier bestehenden Q-Values angepasst.\cite{mnih2013playing}
\newpage
\subsection{Markow-Entscheidungsproblem}
Das Markow-Entscheidungsproblem beschreibt eine Menge von State-Action-Probability-Reward Tupeln. Es gibt also für jeden State, Actions, die eine Chance (Probability) haben in einen anderen State zu wechseln. Jede dieser Transitionen hat einen Reward, der dieser Transition einen Wert zuweist. Das Entscheidungsproblem bezieht sich hier auf die Transition von einem Start-State zu einem Ziel-State, wobei versucht wird den Reward zu maximieren.\cite{Martijn2012}

\subsection{Credit Assignment Problem} \label{cap}
Das Credit Assignment Problem beschreibt ein häufig auftretendes Problem in Environments, welches sich dadurch auszeichnet, dass ein Ablauf aus mehreren Schritten erst nach seinem Abschluss einen Reward zugewiesen bekommt. Es ist dann nicht klar zu beurteilen, welcher der Schritte wie hilfreich zur Erfüllung des Ablaufs war.\cite{agogino2004unifying}

\subsection{Episode}
Eine Episode beschreibt einen einzelnen Durchlauf des Agenten vom Start- bis zum Ziel-State.

\subsection{Epoche}
Eine Epoche ist eine Sammlung von Episoden und beschreibt einen gesammten Lerndurchlauf.


%\colorbox{red!30}{TODO Beispiele} %ausführen



\section{Software}
Um die Problemstellung zu bearbeiten wurde eine Implementierung des Spiels in Python genutzt, welche als ein OpenAI Gym Environment fungiert. An diesem Environment trainiert dann ein Agent, der mit Hilfe von  Keras erschaffen wurde.

\subsection{Python}
Als Programmiersprache wurde sich für Python\cite{python} entschieden, da alle für diese Arbeit wichtigen Packages in dieser Programmiersprache existieren.
Der Schöpfer von Python beschreibt sie selbst als einfach zu erlernende, objektorientierte Programmiersprache, welche Leistung mit einer klaren Syntax verbindet.\cite{PythonManual2009} \\

\newpage
\subsection{OpenAI Gym}
Gym \cite{gym} ist ein Werkzeug zum Entwickeln und Vergleichen von Reinforcement-Learning-Algorithmen, welches von OpenAI, einem Forschungslabor aus San Francisco, zur Verfügung gestellt wird. Es bietet einen Standard zwischen dem Environment und dem Agenten. Hierdurch können neue Agenten und Environments nach gewissen Vorgaben erstellt werden. Somit ist es mit Gym einfacher möglich, verschiedene Agenten an einem Environment zu trainieren und diese zu vergleichen oder den selben Agenten an verschiedenen Environments auf seine Anpassungsfähigkeit zu testen.

\subsection{Tensorflow}
Bei Tensorflow \cite{tensorflow} handelt es sich um ein Open Source Framework für Maschine-Learning, welches ursprünglich für den internen Bedarf bei Google, beispielsweise fürl Spracherkennung oder Google Maps, entwickelt wurde. 

\subsection{Keras}
Keras\cite{keras} ist eine Open Source Bibliothek, die auf Tensorflow aufbaut. Sie bietet die Möglichkeit vereinfacht künstliche neuronale Netze zu erstellen, um diese in Reinforcement-Learning-Algorithmen oder anderen Szenarien zu benutzen.



