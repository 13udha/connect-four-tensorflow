\chapter{Stand der Technik}
\label{cha:Technik}

In den letzten Jahrzehnten wurde für viele Spiele, die bis dahin als zu komplex galten, Programme geschaffen, die diese spielen können. Das Spielniveau der hierdurch erschaffenen Spieler ist dabei gleich gut oder besser als das von den besten menschlichen Spielern.
Dies liegt zu einem großen Teil an den Fortschritten in der Hardware, welche es ermöglichen, große künstliche neronale Netzwerke zu erstellen und zu trainieren. Einige der bekanntesten Erfolge sind zum Beispiel Deep Blue für Schach und Alpha Go für das Spiel Go.\cite{Sutton2018}\\
Weitere Erfolge aus den letzten Jahren sind Agenten, die Computerspiele meistern, wie zum Beispiel MarI/O \cite{mario}, welcher Super Mario spielt oder AlphaStar \cite{alphastar}, welcher Starcraft II spielen kann.
Die Veröffentlichung von OpenAI Gym\cite{gym}, welche unter anderem eine Menge an Atari-Spielen zur Verfügung stellt, eröffnete das Thema Reinforcement-Learning für Spiele für die gesamte Welt. 
Wie in dem Buch von Sutton und Barto \cite{Sutton2018} gezeigt wurde, lassen sich Markow-Entscheidungsprobleme mit Hilfe von reinforcement learning lösen.   
Dass das Lernen durch das Imitieren des Verhalten eines Profis verbessert werden kann, wurde unter anderm von Oliver Amantier gezeigt. \cite{NIPS2016_6391,ARMANTIER2004221,price1999implicit}


\section{Arbeiten mit ähnlichem Inhalt}
Ich habe zwei Arbeiten gefunden, die auch mit Hilfe von Reinforcement-Learning versuchen, das Spiel Vier Gewinnt zu meistern. Einmal die Arbeit von Lukas Stephan \cite{Stephan2018}, welcher das Problem mit einer Java Implementierung angegagen ist und den Lernalgorithmus mit einer Mustererkennung ausgestattet hat. Zweitens die Arbeit von Markus Thill, Patrick Koch und Wolfgang Konen \cite{Thill2012}, welche N-Tupel-Systeme nutzten, um einen Agenten zu trainieren. Anders als diese Arbeiten ist mein Ansatz nicht das Erkennen von Mustern, sondern das Lernen durch das Verhalten des Gegners.









