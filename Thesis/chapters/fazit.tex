\chapter{Fazit}%

\label{cha:Fazit}
Es wurde Festgestellt, dass das Lernen der Züge des Gegners einen positiven Effekt auf das Lernverhalten eines reinforcement learning Agenten haben kann. 
Des weiteren wurde gezeigt, dass die  gewählte Reward-Funktion angemessen war und somit zur Verbesserung des Lernverhalten beigetragen hat.
Dies konnte leider nur am Beispiel von Gegnern mit schlechten und sehr einfach zielführenden Strategien, und nicht am Gegner mit sehr guter Strategie gezeigt werden.
Ob sich dieses Verhalten auch gegen bessere Gegner zeigt müsste dann nochmal durch Vereinfachung der Problemstellung oder durch Verlängerung der Lernepoche getestet werden.
Diese Veränderungen waren auf Grund des Rahmens der Bachelorarbeit leider nicht möglich.


\colorbox{red!30}{TODO}
\section{Besonderheiten}
In den Graphen der Evaluation existiert eine Auffälligkeit die bei ungefähr 250 bis 300 Episoden auftritt. Hiernach wird das Lernverhalten des Agenten oft schlagartig schlechter. Dies ist am deutlichsten in der Abbildung \ref{fig:leftiFF}(b) zu sehen. Ich vermute, dass dies mit der Epsilon-Greedy Policy zusammenhängt die in dem Raum etwa den Maximalen wert für $\epsilon$ erreicht hat.
\colorbox{red!30}{TODO}

%Negamax with alpha beta pruning and transposition tables